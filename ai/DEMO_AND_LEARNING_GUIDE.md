# üéì AI ORCHESTRATOR - DEMO & LEARNING GUIDE

## Overview

This document serves as a **teaching companion** to the AI Orchestrator codebase. It's designed to help you:
- **Understand** the architecture from first principles
- **Demo** the RAG pipeline to others
- **Learn** production AI engineering patterns
- **Extend** the system with new AI concepts

---

## üìã Quick Reference

### System Components
1. **Entry Point**: POST /ai/chat (src/routes/chat.js)
2. **Intent Router**: LLM-based classification (src/router/intentRouter.js)
3. **MCP Tools**: Validated API wrappers (src/mcp/tools/)
4. **RAG Pipeline**: 7-stage document intelligence (src/rag/)
5. **Reconciliation**: Deterministic financial sync (src/reconcile/)

### Key Files Enhanced
- ‚úÖ `AI_ARCHITECTURE_DEEP_DIVE.md` - Comprehensive architectural guide
- ‚úÖ `src/rag/chunker.js` - Educational comments + demo logging
- ‚úÖ `src/rag/embeddings.js` - Model comparisons + cost analysis

---

## üé¨ How to Demo This System

### Demo 1: RAG Pipeline (PDF Q&A)

**Setup**:
1. Start backend: `cd backend && npm start`
2. Start AI orchestrator: `cd ai && npm run dev`
3. Have a sample PDF ready (bank statement, receipt)

**Demo Script**:
```
YOU: "Hi class, today we're learning how RAG works. Let's upload a bank statement."

[Upload PDF via /ai/upload]

SYSTEM LOGS:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üìÑ CHUNKING STAGE - RAG PIPELINE STAGE 2/7‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  üìè Input text length: 5000 characters
  ‚öôÔ∏è  Chunk size: 1500 chars (~375 tokens)
  üîó Overlap size: 200 chars (13%)
  üìä Estimated chunks: ~4
  ‚úì Chunk 0: chars 0-1500 (1500 chars)
    Preview: "Date: January 2026\nTransaction History..."
  ...

YOU: "Notice how it preserves context with 13% overlap. Now let's ask a question."

[Send query: "how much did I spend on hotels?"]

SYSTEM LOGS:
[Similarity Search] User 1 searching for: "how much did I spend on hotels?"
[Similarity Search] Searching across 12 chunks
[Similarity Search] Found 5 results above threshold 0.3
[Similarity Search] Top 3 scores: [0.8721, 0.8103, 0.7856]
[Similarity Search] Best match: 0.8721 from "bank_statement.pdf"

RESPONSE: "Based on your statement, you spent $850 on hotels in January 2026:
- Hotel Grand Stay: $350 (Jan 15)
- Luxury Hotel Downtown: $500 (Jan 22)"

YOU: "See how it retrieved the 3 most relevant chunks, then generated an answer!"
```

**Key Teaching Points**:
- Chunking preserves semantic units
- Embeddings convert text to vectors
- Similarity search finds relevant context
- LLM generates grounded answer

### Demo 2: Intent Classification (Agent-Lite)

**Demo Script**:
```
YOU: "Let's see how the system decides what to do with each message."

[Send: "add 500 for lunch"]
‚Üí Intent: TRANSACTIONAL
‚Üí Handler: Uses MCP tools to call backend API

[Send: "what did my bank statement say about hotels?"]
‚Üí Intent: RAG_QA
‚Üí Handler: Vector search + LLM generation

[Send: "compare my PDF with my tracked expenses"]
‚Üí Intent: RAG_COMPARE
‚Üí Handler: Code-based diff + LLM explanation

YOU: "This is Agent-Lite: ONE LLM decision, then deterministic execution."
```

**Key Teaching Points**:
- Single classification point
- Deterministic routing
- Predictable behavior
- Low cost (1-2 LLM calls per request)

### Demo 3: MCP Tool Pattern (Safety)

**Demo Script**:
```
YOU: "Why don't we let the LLM call the backend API directly? Watch this."

[Send: "add an expense for -$1000"]

SYSTEM LOGS:
[Tool] Executing: create_expense { amount: -1000, ... }
[Validation] ‚ùå Amount must be positive
[Tool] Error returned to LLM

LLM RESPONSE: "I cannot add a negative expense amount. Please provide a positive value."

YOU: "The MCP tool validated BEFORE calling the backend. LLMs can't bypass our rules!"
```

**Key Teaching Points**:
- Tools enforce business logic
- LLM can't bypass validation
- Clear audit trail
- Testable, debuggable

---

## üß† AI Concepts Implemented

### 1. LLM Orchestration
**Location**: src/llm/agent.js
**What**: Using LLMs as reasoning engines with structured I/O
**Key Pattern**: System prompt + tool definitions + conversation history

### 2. Tool Calling (MCP Pattern)
**Location**: src/mcp/tools/
**What**: LLM decides WHAT, tools define HOW
**Key Pattern**: JSON schemas + validation + execution separation

### 3. RAG (Retrieval-Augmented Generation)
**Location**: src/rag/ (7-stage pipeline)
**What**: Ground LLM responses in external knowledge
**Key Pattern**: Chunk ‚Üí Embed ‚Üí Store ‚Üí Search ‚Üí Augment ‚Üí Generate

### 4. Embeddings & Vector Similarity
**Location**: src/rag/embeddings.js, src/rag/search.js
**What**: Convert text to semantic vectors
**Key Pattern**: text-embedding-ada-002 + cosine similarity

### 5. Intent Classification (Agent-Lite)
**Location**: src/router/intentRouter.js
**What**: Single LLM decision ‚Üí deterministic routing
**Key Pattern**: Few-shot prompting + temperature=0.1 + whitelist validation

### 6. Deterministic Financial Logic
**Location**: src/reconcile/
**What**: 100% code-based decisions on money
**Key Pattern**: NO LLM in reconciliation, only in explanation

---

## üöÄ AI Concepts You Can Add (Future)

### 1. Re-Ranking (HIGH IMPACT)
**What**: Two-stage retrieval - fast recall + accurate ranking
**Where**: Add `src/rag/reranker.js`
**Why**: Improves retrieval accuracy from 85% ‚Üí 95%

### 2. Hybrid Search (HIGH IMPACT)
**What**: Combine semantic + keyword search
**Where**: Already in `src/rag/search.js` (hybridSearch function)
**Why**: Handles exact matches better (order IDs, names)

### 3. Query Decomposition (MEDIUM IMPACT)
**What**: Break complex queries into sub-queries
**Where**: Add `src/rag/queryProcessor.js`
**Why**: Better for multi-aspect questions

### 4. Feedback Loops (LONG-TERM)
**What**: Track retrieval quality, improve over time
**Where**: Add `src/eval/metrics.js`
**Why**: Continuous improvement

### 5. Agentic Planning (RISKY)
**What**: Let LLM plan multi-step actions
**Where**: Replace intent router
**Why**: NOT RECOMMENDED - unpredictable, expensive, hard to debug

---

## üìä RAG Pipeline Stages (Detailed)

### Stage 1: PDF Extraction
- **Input**: Binary PDF file
- **Process**: pdf-parse library, multi-tier fallback
- **Output**: Plain text (~5000 chars)
- **Challenge**: Corrupt PDFs, scanned images (OCR needed)

### Stage 2: Chunking
- **Input**: Plain text
- **Process**: Sliding window with overlap
- **Output**: ~12 chunks (1500 chars each, 200 overlap)
- **Challenge**: Infinite loops (fixed with safety guards)

### Stage 3: Embedding Generation
- **Input**: 12 chunks
- **Process**: OpenAI ada-002 API calls
- **Output**: 12 √ó 1536-dimensional vectors
- **Challenge**: Rate limits, timeouts, cost

### Stage 4: Vector Storage
- **Input**: Chunks + embeddings
- **Process**: In-memory storage + JSON persistence
- **Output**: Searchable vector database
- **Challenge**: Scalability (use Pinecone for >1000 docs)

### Stage 5: Query Processing
- **Input**: "how much on hotels?"
- **Process**: Generate query embedding
- **Output**: 1536-dimensional query vector
- **Challenge**: Same as Stage 3

### Stage 6: Similarity Search
- **Input**: Query vector + all document vectors
- **Process**: Cosine similarity computation
- **Output**: Top-5 most similar chunks (scores: 0.87, 0.81, 0.79, ...)
- **Challenge**: O(n) linear scan (use ANN for scale)

### Stage 7: LLM Generation
- **Input**: Query + top-5 chunks as context
- **Process**: GPT-4o-mini with temperature=0.7
- **Output**: Natural language answer grounded in context
- **Challenge**: Hallucination prevention

---

## üéì Learning Exercises

### Exercise 1: Understand Chunking
1. Read `src/rag/chunker.js`
2. Change chunk size to 500 (too small)
3. Upload PDF, note retrieval quality drops
4. Change to 3000 (too large)
5. Note mixed-topic chunks reduce precision
6. Restore to 1500 (optimal)

### Exercise 2: Experiment with Similarity Threshold
1. Read `src/rag/search.js`
2. Change minSimilarity from 0.3 ‚Üí 0.7 (strict)
3. Query: "hotel costs"
4. Note fewer results (high precision, low recall)
5. Change to 0.1 (loose)
6. Note more results (low precision, high recall)

### Exercise 3: Add Hybrid Search
1. Read `src/rag/search.js` line 120 (hybridSearch function)
2. Expose it in `src/handlers/ragQaHandler.js`
3. Test query: "Order #12345"
4. Compare pure vector vs hybrid retrieval

---

## üè≠ Production Checklist

### Scaling Decisions

| Metric | In-Memory (Now) | Need Vector DB | Need Distributed |
|--------|-----------------|----------------|-------------------|
| Users | <100 | 100-10K | 10K+ |
| Documents | <1,000 | 1K-100K | 100K+ |
| Queries/sec | <10 | 10-100 | 100+ |

### Cost Optimization
- ‚úÖ Rate limiting (100 req/15min)
- ‚úÖ Vector persistence (no re-embedding)
- ‚ö†Ô∏è Add: LLM response caching
- ‚ö†Ô∏è Add: Embedding caching

### Monitoring Needs
- Latency (P50, P95, P99)
- LLM token usage per user
- Similarity score distributions
- Error rates (timeout, validation)

---

## üìö Additional Resources

### Papers to Read
1. **RAG**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)
2. **Embeddings**: "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" (Reimers & Gurevych, 2019)
3. **Tool Use**: "Toolformer: Language Models Can Teach Themselves to Use Tools" (Schick et al., 2023)

### Industry Blogs
- Pinecone: RAG best practices
- OpenAI: Embeddings guide
- LangChain: Agent patterns

### Similar Systems
- LangChain (framework)
- LlamaIndex (RAG-focused)
- Haystack (NLP pipelines)

---

## üéØ Teaching This to Others

### 30-Minute Workshop
1. **Overview** (5 min): Three-tier architecture
2. **Live Demo** (10 min): Upload PDF ‚Üí Ask question
3. **Code Walkthrough** (10 min): RAG pipeline stages
4. **Q&A** (5 min)

### 2-Hour Deep Dive
1. **Architecture** (20 min): Intent router + handlers
2. **RAG Pipeline** (40 min): Each stage in detail
3. **MCP Pattern** (20 min): Tool calling + validation
4. **Live Coding** (30 min): Add hybrid search
5. **Q&A** (10 min)

### Key Messages
- ‚úÖ RAG grounds LLMs in facts (reduces hallucination)
- ‚úÖ Agent-Lite is better than Full Agent for production
- ‚úÖ Deterministic logic for financial decisions
- ‚úÖ User isolation is critical in multi-tenant AI

---

**Document Version**: 1.0  
**Last Updated**: February 2, 2026  
**Maintainer**: AI Orchestrator Team  
**Status**: Ready for Educational Use
