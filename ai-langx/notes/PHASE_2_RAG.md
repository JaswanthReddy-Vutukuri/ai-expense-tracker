# Phase 2: RAG Pipeline - COMPLETE âœ…

## Overview
Phase 2 implements a complete **Retrieval-Augmented Generation (RAG)** pipeline using LangChain components. This enables PDF document upload, semantic search, and question answering based on user's uploaded documents.

## Architecture

```
PDF Upload â†’ Document Loading â†’ Text Splitting â†’ Embedding â†’ Vector Storage â†’ Retrieval â†’ QA
```

## Components Implemented

### 1. Document Loading
**File**: [`src/rag/loaders/pdf.loader.js`](../src/rag/loaders/pdf.loader.js)

- Uses LangChain `PDFLoader` via `pdf-parse`
- Loads from buffer (uploads) or file path
- Creates `Document` objects with metadata
- Validates content length and structure

**Key Functions**:
- `loadPDFFromBuffer(buffer, metadata)` - Load uploaded PDF
- `loadPDFFromPath(filePath, metadata)` - Load from disk
- `validatePDFContent(documents)` - Content validation

**Advantages over custom**:
- LangChain Document abstraction
- Automatic metadata preservation
- ~50% less code

### 2. Text Splitting
**File**: [`src/rag/splitters/text.splitter.js`](../src/rag/splitters/text.splitter.js)

- Uses `RecursiveCharacterTextSplitter`
- Semantic splitting (paragraphs â†’ sentences â†’ words)
- 500 character chunks with 50 character overlap
- Preserves metadata and adds chunk indices

**Key Functions**:
- `splitDocuments(documents)` - Split with overlap
- `splitText(text, metadata)` - Quick text splitting

**Advantages over custom**:
- Semantic splitting (respects boundaries)
- Optimized chunk size
- ~60% less code

### 3. Embeddings
**File**: [`src/rag/embeddings/openai.embeddings.js`](../src/rag/embeddings/openai.embeddings.js)

- Uses `OpenAIEmbeddings` with `text-embedding-ada-002`
- Auto-batching (512 batch size)
- Timeout handling (15s)
- Generates 1536-dimensional vectors

**Key Functions**:
- `generateEmbedding(text)` - Single embedding
- `generateEmbeddings(texts)` - Batch embeddings

**Advantages over custom**:
- Automatic batching
- Built-in retries
- ~70% less code

### 4. Vector Store
**File**: [`src/rag/vectorstore/memory.store.js`](../src/rag/vectorstore/memory.store.js)

- Uses `MemoryVectorStore` for fast in-memory search
- Manual persistence to JSON for durability
- User isolation via metadata filtering
- GDPR-compliant deletion

**Key Functions**:
- `getVectorStore()` - Get/create store
- `addDocuments(documents, metadata)` - Add with auto-embedding
- `similaritySearch(query, k, filter)` - Semantic search
- `deleteUserDocuments(userId)` - User data deletion
- `getStats()` - Store statistics

**Advantages over custom**:
- No manual cosine similarity
- Optimized search algorithms
- Easy swap to production DBs (Pinecone, Weaviate)
- ~75% less code

### 5. Retrieval
**File**: [`src/rag/retrievers/user.retriever.js`](../src/rag/retrievers/user.retriever.js)

- Uses `VectorStoreRetriever` interface
- User filtering for multi-tenancy
- Score threshold filtering
- Context formatting for LLM

**Key Functions**:
- `createUserRetriever(userId, k)` - Create filtered retriever
- `retrieveDocuments(query, userId, k)` - Retrieve relevant docs
- `retrieveWithThreshold(query, userId, threshold, k)` - Filter by score
- `formatDocumentsForContext(documents)` - Format for LLM prompt

**Advantages over custom**:
- Standard interface for chains
- Easy to add MMR, reranking
- ~50% less code

### 6. QA Chain
**File**: [`src/rag/chains/qa.chain.js`](../src/rag/chains/qa.chain.js)

- Uses `RetrievalQAChain` for automatic RAG
- Custom prompt template for PDF context
- Source attribution
- Streaming support
- Multi-query support (advanced)

**Key Functions**:
- `createQAChain(userId, options)` - Create RAG chain
- `answerQuestion(question, userId, options)` - Answer with sources
- `answerQuestionStreaming(question, userId, onToken)` - Stream response
- `answerWithMultiQuery(question, userId)` - Multi-query RAG

**Advantages over custom**:
- Auto retrieval + generation pipeline
- Built-in prompt templates
- Easy conversation history support
- ~80% less code

### 7. Upload Route
**File**: [`src/routes/upload.js`](../src/routes/upload.js)

- `POST /ai/upload` - Upload PDF
- `GET /ai/upload/documents` - List user's documents
- `DELETE /ai/upload/documents` - Delete all user documents

**Features**:
- Multer file handling (10MB max)
- Automatic pipeline: PDF â†’ Load â†’ Split â†’ Embed â†’ Store
- User metadata isolation
- Comprehensive error handling

**Advantages over custom**:
- Single pipeline call (no manual loops)
- Better error messages
- ~60% less code

### 8. RAG Handler
**File**: [`src/handlers/rag.handler.js`](../src/handlers/rag.handler.js)

- Integrates RAG with chat system
- Document availability checking
- Source formatting
- Streaming support
- PDF comparison feature

**Key Functions**:
- `handleRAGQuestion(question, userId, options)` - Answer question
- `handleRAGQuestionStreaming(question, userId, onToken)` - Stream answer
- `handleRAGPreview(query, userId, k)` - Preview retrieval
- `handleRAGCompare(expenses, userId)` - Compare expenses with PDFs

## Integration

### Server Setup
Updated [`server.js`](../server.js):
```javascript
import uploadRoutes from './src/routes/upload.js';
app.use('/ai/upload', authMiddleware, uploadRoutes);
```

### Chat Route Integration
Updated [`src/routes/chat.js`](../src/routes/chat.js):
- Detects RAG questions (mentions PDF/document + question word)
- Routes to `handleRAGQuestion()` automatically
- Falls back to expense agent for non-RAG queries

## Usage Examples

### 1. Upload PDF
```bash
curl -X POST http://localhost:3002/ai/upload \
  -H "Authorization: Bearer $TOKEN" \
  -F "file=@receipt.pdf"
```

Response:
```json
{
  "success": true,
  "message": "PDF uploaded and processed successfully",
  "data": {
    "filename": "receipt.pdf",
    "pages": 3,
    "chunks": 15,
    "vectorIds": 15
  }
}
```

### 2. Ask Question
```bash
curl -X POST http://localhost:3002/ai/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "What does my PDF say about lunch expenses?"}'
```

Response:
```json
{
  "reply": "According to your uploaded receipt, you spent â‚¹500 on lunch at Restaurant XYZ on January 15th.\n\nðŸ“„ Sources:\n1. receipt.pdf (page 1)"
}
```

### 3. List Documents
```bash
curl http://localhost:3002/ai/upload/documents \
  -H "Authorization: Bearer $TOKEN"
```

### 4. Delete Documents
```bash
curl -X DELETE http://localhost:3002/ai/upload/documents \
  -H "Authorization: Bearer $TOKEN"
```

## Code Comparison

### Custom vs LangChain

**Custom Implementation (ai/):**
```javascript
// ~800 LOC across multiple files
const pdfData = await pdfParse(buffer);
const chunks = chunkText(pdfData.text, 500);
for (const chunk of chunks) {
  const embedding = await generateEmbedding(chunk);
  await vectorStore.add({ text: chunk, embedding, userId });
}
const searchResults = vectorStore.search(queryEmbedding, 5);
const context = searchResults.map(r => r.text).join('\n');
const response = await openai.chat.completions.create({...});
```

**LangChain Implementation (ai-langx/):**
```javascript
// ~400 LOC across multiple files
const documents = await loadPDFFromBuffer(buffer, metadata);
const chunks = await splitDocuments(documents);
await addDocuments(chunks);  // Auto-embeds!
const result = await answerQuestion(question, userId);
```

**Reduction**: ~50-80% less code per component

## Benefits of LangChain Approach

### Development Speed
- âœ… Pre-built components (loaders, splitters, embeddings)
- âœ… Automatic pipeline orchestration
- âœ… No manual embedding loops
- âœ… Built-in batching and optimization

### Flexibility
- âœ… Easy to swap PDF loader (Unstructured, PyPDF, etc.)
- âœ… Easy to swap vector store (Pinecone, Weaviate, Chroma)
- âœ… Easy to add reranking, MMR, hybrid search
- âœ… Easy to change embedding models

### Advanced Features
- âœ… Conversational RAG (with history)
- âœ… Multi-query retrieval (query expansion)
- âœ… HyDE (hypothetical document embeddings)
- âœ… MapReduce chains (long documents)
- âœ… Parent document retrieval

### Production Ready
- âœ… Optimized search algorithms
- âœ… Built-in error handling
- âœ… Metadata filtering
- âœ… Score thresholding
- âœ… Context window management

## Advanced Patterns Available

### 1. Conversational RAG
```javascript
import { ConversationalRetrievalQAChain } from "langchain/chains";
const chain = ConversationalRetrievalQAChain.fromLLM(llm, retriever);
const result = await chain.call({
  question,
  chat_history: previousMessages
});
```

### 2. Multi-Query Retrieval
```javascript
import { MultiQueryRetriever } from "langchain/retrievers/multi_query";
// Generates multiple query variations automatically
```

### 3. Reranking
```javascript
import { CohereRerank } from "@langchain/cohere";
const reranker = new CohereRerank();
const reranked = await reranker.rerank(docs, query);
```

### 4. Hybrid Search
- Combine vector similarity with keyword search
- Available in Weaviate, Pinecone

## Testing

### Manual Testing
1. **Start server**: `npm run dev` (port 3002)
2. **Get token**: Use backend auth endpoint
3. **Upload PDF**: Use curl or Postman
4. **Ask question**: Via chat endpoint

### Key Test Cases
- âœ… Upload valid PDF
- âœ… Upload invalid file (not PDF)
- âœ… Upload large PDF (>10MB)
- âœ… Upload PDF with no text (scanned)
- âœ… Ask question before upload
- âœ… Ask question after upload
- âœ… Multiple PDFs for same user
- âœ… User isolation (can't see other users' PDFs)
- âœ… Delete all documents
- âœ… GDPR compliance

## Next Steps: Phase 3

Phase 3 will implement **LangGraph workflows** for:

1. **Intent Routing Graph**
   - Multi-step decision making
   - Conditional routing
   - State management

2. **Reconciliation Graph**
   - Multi-stage reconciliation
   - Parallel processing
   - Error recovery

See [`notes/AGENT_PLANNER.md`](../../notes/AGENT_PLANNER.md) for details.

## Files Created (Phase 2)

```
ai-langx/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â”‚   â””â”€â”€ pdf.loader.js
â”‚   â”‚   â”œâ”€â”€ splitters/
â”‚   â”‚   â”‚   â””â”€â”€ text.splitter.js
â”‚   â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”‚   â””â”€â”€ openai.embeddings.js
â”‚   â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â”‚   â””â”€â”€ memory.store.js
â”‚   â”‚   â”œâ”€â”€ retrievers/
â”‚   â”‚   â”‚   â””â”€â”€ user.retriever.js
â”‚   â”‚   â””â”€â”€ chains/
â”‚   â”‚       â””â”€â”€ qa.chain.js
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â””â”€â”€ rag.handler.js
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ upload.js
â””â”€â”€ data/
    â””â”€â”€ vectorstore/
        â””â”€â”€ README.md
```

**Total**: 9 new files, ~1,500 LOC (vs ~3,000 LOC custom)

## Comparison Summary

| Feature | Custom (ai/) | LangChain (ai-langx/) |
|---------|--------------|------------------------|
| **Code Volume** | ~3,000 LOC | ~1,500 LOC |
| **PDF Loading** | Manual pdf-parse | LangChain PDFLoader |
| **Text Splitting** | Simple char split | Semantic RecursiveTextSplitter |
| **Embeddings** | Manual OpenAI calls | OpenAIEmbeddings |
| **Vector Store** | Manual cosine similarity | MemoryVectorStore |
| **Search** | Manual ranking | Optimized similarity |
| **QA Pipeline** | Manual prompt building | RetrievalQAChain |
| **Flexibility** | Hard to swap components | Easy component swap |
| **Advanced Features** | Need manual implementation | Built-in (MMR, rerank, etc.) |

## Deployment Notes

### Environment Variables
```bash
OPENAI_API_KEY=sk-...
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__...
```

### Production Vector Store
Replace `MemoryVectorStore` with managed solution:

```javascript
// Pinecone
import { Pinecone } from "@langchain/community/vectorstores/pinecone";
const store = await Pinecone.fromDocuments(docs, embeddings, {
  pineconeIndex: index,
  namespace: userId.toString()
});

// Same interface, no other code changes!
```

### Background Processing
For large PDFs, use task queue:
```javascript
import { Queue } from 'bullmq';
const pdfQueue = new Queue('pdf-processing');
```

## Learning Resources

- [LangChain RAG Tutorial](https://js.langchain.com/docs/use_cases/question_answering/)
- [Vector Store Comparison](https://js.langchain.com/docs/modules/data_connection/vectorstores/)
- [Document Loaders](https://js.langchain.com/docs/modules/data_connection/document_loaders/)
- [Text Splitters](https://js.langchain.com/docs/modules/data_connection/document_transformers/)

---

**Phase 2 Status**: âœ… **COMPLETE**  
**Next Phase**: Phase 3 - LangGraph Workflows
